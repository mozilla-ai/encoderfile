pub const MODEL_TYPE_STR: &str = env!("MODEL_TYPE");
pub const MODEL_WEIGHTS: &[u8] = include_bytes!(env!("MODEL_WEIGHTS_PATH"));
pub const TOKENIZER_JSON: &str = include_str!(env!("TOKENIZER_PATH"));
pub const MODEL_CONFIG_JSON: &str = include_str!(env!("MODEL_CONFIG_PATH"));

pub const BANNER: &'static str = include_str!("../assets/banner.txt");
pub const MODEL_ID: &'static str = env!("MODEL_NAME");

pub fn get_banner() -> String {
    let model_id_len = MODEL_ID.len();
    let signature = " | Mozilla.ai";
    let total_len: usize = 73;
    let remaining_len = total_len - model_id_len - signature.len();

    let spaces = " ".repeat(remaining_len);

    format!(
        "{}\nModel ID: {}{}{}\n",
        BANNER, MODEL_ID, spaces, signature
    )
}
use clap_derive::{Parser, Subcommand};

#[derive(Parser)]
pub struct Cli {
    #[command(subcommand)]
    pub command: Commands,
}

#[derive(Subcommand)]
pub enum Commands {
    Serve {
        #[arg(long, default_value = "[::]")]
        grpc_hostname: String,
        #[arg(long, default_value = "50051")]
        grpc_port: String,
        #[arg(long, default_value = "0.0.0.0")]
        http_hostname: String,
        #[arg(long, default_value = "8080")]
        http_port: String,
        #[arg(long, default_value_t = false)]
        disable_grpc: bool,
        #[arg(long, default_value_t = false)]
        disable_http: bool,
    },
    Infer {
        #[arg(required = true)]
        inputs: Vec<String>,
        #[arg(long, default_value_t = true)]
        normalize: bool,
    },
}

#[derive(Subcommand)]
pub enum ServeCommands {
    Grpc {
        #[arg(long, default_value = "[::]")]
        hostname: String,
        #[arg(long, default_value = "50051")]
        port: String,
    },
    Http {
        #[arg(long, default_value = "0.0.0.0")]
        hostname: String,
        #[arg(long, default_value = "8080")]
        port: String,
    },
}
use std::{collections::HashMap, sync::OnceLock};

use crate::assets::{MODEL_CONFIG_JSON, MODEL_TYPE_STR};

use serde::{Deserialize, Serialize};

static MODEL_CONFIG: OnceLock<ModelConfig> = OnceLock::new();
static MODEL_TYPE: OnceLock<ModelType> = OnceLock::new();

pub fn get_model_config() -> &'static ModelConfig {
    MODEL_CONFIG.get_or_init(
        || match serde_json::from_str::<ModelConfig>(MODEL_CONFIG_JSON) {
            Ok(c) => c,
            Err(e) => panic!("FATAL: Error loading model config: {e:?}"),
        },
    )
}

pub fn get_model_type() -> &'static ModelType {
    MODEL_TYPE.get_or_init(|| match MODEL_TYPE_STR {
        "embedding" => ModelType::Embedding,
        "sequence_classification" => ModelType::SequenceClassification,
        "token_classification" => ModelType::TokenClassification,
        other => panic!("Invalid model type: {}", other),
    })
}

#[derive(Debug)]
pub enum ModelType {
    Embedding,
    SequenceClassification,
    TokenClassification,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct ModelConfig {
    pub model_type: String,
    pub pad_token_id: u32,
    pub id2label: Option<HashMap<u32, String>>,
    pub label2id: Option<HashMap<String, u32>>,
}

impl ModelConfig {
    pub fn id2label(&self, id: u32) -> Option<&str> {
        self.id2label.as_ref()?.get(&id).map(|s| s.as_str())
    }

    pub fn label2id(&self, label: &str) -> Option<u32> {
        self.label2id.as_ref()?.get(label).map(|i| *i)
    }
}
use axum::http::StatusCode;
use serde::Serialize;
use thiserror::Error;
use tonic::Status;

#[derive(Debug, Error, Serialize)]
pub enum ApiError {
    #[error("Invalid Input: {0}")]
    InputError(&'static str),

    #[error("Internal Error: {0}")]
    InternalError(&'static str),

    #[error("Config Error: {0}")]
    ConfigError(&'static str),
}

impl ApiError {
    pub fn to_tonic_status(&self) -> Status {
        match self {
            Self::InputError(s) => Status::invalid_argument(*s),
            Self::InternalError(s) => Status::internal(*s),
            Self::ConfigError(s) => Status::internal(*s),
        }
    }

    pub fn to_axum_status(&self) -> (StatusCode, &'static str) {
        match self {
            Self::InputError(s) => (StatusCode::BAD_REQUEST, *s),
            Self::InternalError(s) => (StatusCode::INTERNAL_SERVER_ERROR, *s),
            Self::ConfigError(s) => (StatusCode::INTERNAL_SERVER_ERROR, *s),
        }
    }
}
// This file is @generated by prost-build.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EmbeddingRequest {
    #[prost(string, repeated, tag = "1")]
    pub inputs: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    #[prost(bool, tag = "2")]
    pub normalize: bool,
    #[prost(map = "string, string", tag = "3")]
    pub metadata: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct EmbeddingResponse {
    /// len(embeddings) == len(inputs)
    #[prost(message, repeated, tag = "1")]
    pub results: ::prost::alloc::vec::Vec<TokenEmbeddingSequence>,
    #[prost(string, tag = "2")]
    pub model_id: ::prost::alloc::string::String,
    #[prost(map = "string, string", tag = "3")]
    pub metadata: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TokenEmbeddingSequence {
    #[prost(message, repeated, tag = "1")]
    pub embeddings: ::prost::alloc::vec::Vec<TokenEmbedding>,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TokenEmbedding {
    #[prost(float, repeated, tag = "1")]
    pub embedding: ::prost::alloc::vec::Vec<f32>,
    #[prost(message, optional, tag = "2")]
    pub token_info: ::core::option::Option<super::token::TokenInfo>,
}
// This file is @generated by prost-build.
/// Generated client implementations.
pub mod embedding_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    #[derive(Debug, Clone)]
    pub struct EmbeddingClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl EmbeddingClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> EmbeddingClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::Body>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> EmbeddingClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::Body>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::Body>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::Body>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            EmbeddingClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        pub async fn predict(
            &mut self,
            request: impl tonic::IntoRequest<super::super::embedding::EmbeddingRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::embedding::EmbeddingResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic_prost::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/encoderfile.Embedding/Predict",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("encoderfile.Embedding", "Predict"));
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Generated server implementations.
pub mod embedding_server {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    /// Generated trait containing gRPC methods that should be implemented for use with EmbeddingServer.
    #[async_trait]
    pub trait Embedding: std::marker::Send + std::marker::Sync + 'static {
        async fn predict(
            &self,
            request: tonic::Request<super::super::embedding::EmbeddingRequest>,
        ) -> std::result::Result<
            tonic::Response<super::super::embedding::EmbeddingResponse>,
            tonic::Status,
        >;
    }
    #[derive(Debug)]
    pub struct EmbeddingServer<T> {
        inner: Arc<T>,
        accept_compression_encodings: EnabledCompressionEncodings,
        send_compression_encodings: EnabledCompressionEncodings,
        max_decoding_message_size: Option<usize>,
        max_encoding_message_size: Option<usize>,
    }
    impl<T> EmbeddingServer<T> {
        pub fn new(inner: T) -> Self {
            Self::from_arc(Arc::new(inner))
        }
        pub fn from_arc(inner: Arc<T>) -> Self {
            Self {
                inner,
                accept_compression_encodings: Default::default(),
                send_compression_encodings: Default::default(),
                max_decoding_message_size: None,
                max_encoding_message_size: None,
            }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> InterceptedService<Self, F>
        where
            F: tonic::service::Interceptor,
        {
            InterceptedService::new(Self::new(inner), interceptor)
        }
        /// Enable decompressing requests with the given encoding.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.accept_compression_encodings.enable(encoding);
            self
        }
        /// Compress responses with the given encoding, if the client supports it.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.send_compression_encodings.enable(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.max_decoding_message_size = Some(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.max_encoding_message_size = Some(limit);
            self
        }
    }
    impl<T, B> tonic::codegen::Service<http::Request<B>> for EmbeddingServer<T>
    where
        T: Embedding,
        B: Body + std::marker::Send + 'static,
        B::Error: Into<StdError> + std::marker::Send + 'static,
    {
        type Response = http::Response<tonic::body::Body>;
        type Error = std::convert::Infallible;
        type Future = BoxFuture<Self::Response, Self::Error>;
        fn poll_ready(
            &mut self,
            _cx: &mut Context<'_>,
        ) -> Poll<std::result::Result<(), Self::Error>> {
            Poll::Ready(Ok(()))
        }
        fn call(&mut self, req: http::Request<B>) -> Self::Future {
            match req.uri().path() {
                "/encoderfile.Embedding/Predict" => {
                    #[allow(non_camel_case_types)]
                    struct PredictSvc<T: Embedding>(pub Arc<T>);
                    impl<
                        T: Embedding,
                    > tonic::server::UnaryService<
                        super::super::embedding::EmbeddingRequest,
                    > for PredictSvc<T> {
                        type Response = super::super::embedding::EmbeddingResponse;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<
                                super::super::embedding::EmbeddingRequest,
                            >,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as Embedding>::predict(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = PredictSvc(inner);
                        let codec = tonic_prost::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                _ => {
                    Box::pin(async move {
                        let mut response = http::Response::new(
                            tonic::body::Body::default(),
                        );
                        let headers = response.headers_mut();
                        headers
                            .insert(
                                tonic::Status::GRPC_STATUS,
                                (tonic::Code::Unimplemented as i32).into(),
                            );
                        headers
                            .insert(
                                http::header::CONTENT_TYPE,
                                tonic::metadata::GRPC_CONTENT_TYPE,
                            );
                        Ok(response)
                    })
                }
            }
        }
    }
    impl<T> Clone for EmbeddingServer<T> {
        fn clone(&self) -> Self {
            let inner = self.inner.clone();
            Self {
                inner,
                accept_compression_encodings: self.accept_compression_encodings,
                send_compression_encodings: self.send_compression_encodings,
                max_decoding_message_size: self.max_decoding_message_size,
                max_encoding_message_size: self.max_encoding_message_size,
            }
        }
    }
    /// Generated gRPC service name
    pub const SERVICE_NAME: &str = "encoderfile.Embedding";
    impl<T> tonic::server::NamedService for EmbeddingServer<T> {
        const NAME: &'static str = SERVICE_NAME;
    }
}
/// Generated client implementations.
pub mod sequence_classification_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    #[derive(Debug, Clone)]
    pub struct SequenceClassificationClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl SequenceClassificationClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> SequenceClassificationClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::Body>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> SequenceClassificationClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::Body>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::Body>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::Body>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            SequenceClassificationClient::new(
                InterceptedService::new(inner, interceptor),
            )
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        pub async fn predict(
            &mut self,
            request: impl tonic::IntoRequest<
                super::super::sequence_classification::SequenceClassificationRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<
                super::super::sequence_classification::SequenceClassificationResponse,
            >,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic_prost::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/encoderfile.SequenceClassification/Predict",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("encoderfile.SequenceClassification", "Predict"),
                );
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Generated server implementations.
pub mod sequence_classification_server {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    /// Generated trait containing gRPC methods that should be implemented for use with SequenceClassificationServer.
    #[async_trait]
    pub trait SequenceClassification: std::marker::Send + std::marker::Sync + 'static {
        async fn predict(
            &self,
            request: tonic::Request<
                super::super::sequence_classification::SequenceClassificationRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<
                super::super::sequence_classification::SequenceClassificationResponse,
            >,
            tonic::Status,
        >;
    }
    #[derive(Debug)]
    pub struct SequenceClassificationServer<T> {
        inner: Arc<T>,
        accept_compression_encodings: EnabledCompressionEncodings,
        send_compression_encodings: EnabledCompressionEncodings,
        max_decoding_message_size: Option<usize>,
        max_encoding_message_size: Option<usize>,
    }
    impl<T> SequenceClassificationServer<T> {
        pub fn new(inner: T) -> Self {
            Self::from_arc(Arc::new(inner))
        }
        pub fn from_arc(inner: Arc<T>) -> Self {
            Self {
                inner,
                accept_compression_encodings: Default::default(),
                send_compression_encodings: Default::default(),
                max_decoding_message_size: None,
                max_encoding_message_size: None,
            }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> InterceptedService<Self, F>
        where
            F: tonic::service::Interceptor,
        {
            InterceptedService::new(Self::new(inner), interceptor)
        }
        /// Enable decompressing requests with the given encoding.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.accept_compression_encodings.enable(encoding);
            self
        }
        /// Compress responses with the given encoding, if the client supports it.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.send_compression_encodings.enable(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.max_decoding_message_size = Some(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.max_encoding_message_size = Some(limit);
            self
        }
    }
    impl<T, B> tonic::codegen::Service<http::Request<B>>
    for SequenceClassificationServer<T>
    where
        T: SequenceClassification,
        B: Body + std::marker::Send + 'static,
        B::Error: Into<StdError> + std::marker::Send + 'static,
    {
        type Response = http::Response<tonic::body::Body>;
        type Error = std::convert::Infallible;
        type Future = BoxFuture<Self::Response, Self::Error>;
        fn poll_ready(
            &mut self,
            _cx: &mut Context<'_>,
        ) -> Poll<std::result::Result<(), Self::Error>> {
            Poll::Ready(Ok(()))
        }
        fn call(&mut self, req: http::Request<B>) -> Self::Future {
            match req.uri().path() {
                "/encoderfile.SequenceClassification/Predict" => {
                    #[allow(non_camel_case_types)]
                    struct PredictSvc<T: SequenceClassification>(pub Arc<T>);
                    impl<
                        T: SequenceClassification,
                    > tonic::server::UnaryService<
                        super::super::sequence_classification::SequenceClassificationRequest,
                    > for PredictSvc<T> {
                        type Response = super::super::sequence_classification::SequenceClassificationResponse;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<
                                super::super::sequence_classification::SequenceClassificationRequest,
                            >,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as SequenceClassification>::predict(&inner, request)
                                    .await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = PredictSvc(inner);
                        let codec = tonic_prost::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                _ => {
                    Box::pin(async move {
                        let mut response = http::Response::new(
                            tonic::body::Body::default(),
                        );
                        let headers = response.headers_mut();
                        headers
                            .insert(
                                tonic::Status::GRPC_STATUS,
                                (tonic::Code::Unimplemented as i32).into(),
                            );
                        headers
                            .insert(
                                http::header::CONTENT_TYPE,
                                tonic::metadata::GRPC_CONTENT_TYPE,
                            );
                        Ok(response)
                    })
                }
            }
        }
    }
    impl<T> Clone for SequenceClassificationServer<T> {
        fn clone(&self) -> Self {
            let inner = self.inner.clone();
            Self {
                inner,
                accept_compression_encodings: self.accept_compression_encodings,
                send_compression_encodings: self.send_compression_encodings,
                max_decoding_message_size: self.max_decoding_message_size,
                max_encoding_message_size: self.max_encoding_message_size,
            }
        }
    }
    /// Generated gRPC service name
    pub const SERVICE_NAME: &str = "encoderfile.SequenceClassification";
    impl<T> tonic::server::NamedService for SequenceClassificationServer<T> {
        const NAME: &'static str = SERVICE_NAME;
    }
}
/// Generated client implementations.
pub mod token_classification_client {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    use tonic::codegen::http::Uri;
    #[derive(Debug, Clone)]
    pub struct TokenClassificationClient<T> {
        inner: tonic::client::Grpc<T>,
    }
    impl TokenClassificationClient<tonic::transport::Channel> {
        /// Attempt to create a new client by connecting to a given endpoint.
        pub async fn connect<D>(dst: D) -> Result<Self, tonic::transport::Error>
        where
            D: TryInto<tonic::transport::Endpoint>,
            D::Error: Into<StdError>,
        {
            let conn = tonic::transport::Endpoint::new(dst)?.connect().await?;
            Ok(Self::new(conn))
        }
    }
    impl<T> TokenClassificationClient<T>
    where
        T: tonic::client::GrpcService<tonic::body::Body>,
        T::Error: Into<StdError>,
        T::ResponseBody: Body<Data = Bytes> + std::marker::Send + 'static,
        <T::ResponseBody as Body>::Error: Into<StdError> + std::marker::Send,
    {
        pub fn new(inner: T) -> Self {
            let inner = tonic::client::Grpc::new(inner);
            Self { inner }
        }
        pub fn with_origin(inner: T, origin: Uri) -> Self {
            let inner = tonic::client::Grpc::with_origin(inner, origin);
            Self { inner }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> TokenClassificationClient<InterceptedService<T, F>>
        where
            F: tonic::service::Interceptor,
            T::ResponseBody: Default,
            T: tonic::codegen::Service<
                http::Request<tonic::body::Body>,
                Response = http::Response<
                    <T as tonic::client::GrpcService<tonic::body::Body>>::ResponseBody,
                >,
            >,
            <T as tonic::codegen::Service<
                http::Request<tonic::body::Body>,
            >>::Error: Into<StdError> + std::marker::Send + std::marker::Sync,
        {
            TokenClassificationClient::new(InterceptedService::new(inner, interceptor))
        }
        /// Compress requests with the given encoding.
        ///
        /// This requires the server to support it otherwise it might respond with an
        /// error.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.send_compressed(encoding);
            self
        }
        /// Enable decompressing responses.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.inner = self.inner.accept_compressed(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_decoding_message_size(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.inner = self.inner.max_encoding_message_size(limit);
            self
        }
        pub async fn predict(
            &mut self,
            request: impl tonic::IntoRequest<
                super::super::token_classification::TokenClassificationRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<
                super::super::token_classification::TokenClassificationResponse,
            >,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::unknown(
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic_prost::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/encoderfile.TokenClassification/Predict",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(GrpcMethod::new("encoderfile.TokenClassification", "Predict"));
            self.inner.unary(req, path, codec).await
        }
    }
}
/// Generated server implementations.
pub mod token_classification_server {
    #![allow(
        unused_variables,
        dead_code,
        missing_docs,
        clippy::wildcard_imports,
        clippy::let_unit_value,
    )]
    use tonic::codegen::*;
    /// Generated trait containing gRPC methods that should be implemented for use with TokenClassificationServer.
    #[async_trait]
    pub trait TokenClassification: std::marker::Send + std::marker::Sync + 'static {
        async fn predict(
            &self,
            request: tonic::Request<
                super::super::token_classification::TokenClassificationRequest,
            >,
        ) -> std::result::Result<
            tonic::Response<
                super::super::token_classification::TokenClassificationResponse,
            >,
            tonic::Status,
        >;
    }
    #[derive(Debug)]
    pub struct TokenClassificationServer<T> {
        inner: Arc<T>,
        accept_compression_encodings: EnabledCompressionEncodings,
        send_compression_encodings: EnabledCompressionEncodings,
        max_decoding_message_size: Option<usize>,
        max_encoding_message_size: Option<usize>,
    }
    impl<T> TokenClassificationServer<T> {
        pub fn new(inner: T) -> Self {
            Self::from_arc(Arc::new(inner))
        }
        pub fn from_arc(inner: Arc<T>) -> Self {
            Self {
                inner,
                accept_compression_encodings: Default::default(),
                send_compression_encodings: Default::default(),
                max_decoding_message_size: None,
                max_encoding_message_size: None,
            }
        }
        pub fn with_interceptor<F>(
            inner: T,
            interceptor: F,
        ) -> InterceptedService<Self, F>
        where
            F: tonic::service::Interceptor,
        {
            InterceptedService::new(Self::new(inner), interceptor)
        }
        /// Enable decompressing requests with the given encoding.
        #[must_use]
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.accept_compression_encodings.enable(encoding);
            self
        }
        /// Compress responses with the given encoding, if the client supports it.
        #[must_use]
        pub fn send_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.send_compression_encodings.enable(encoding);
            self
        }
        /// Limits the maximum size of a decoded message.
        ///
        /// Default: `4MB`
        #[must_use]
        pub fn max_decoding_message_size(mut self, limit: usize) -> Self {
            self.max_decoding_message_size = Some(limit);
            self
        }
        /// Limits the maximum size of an encoded message.
        ///
        /// Default: `usize::MAX`
        #[must_use]
        pub fn max_encoding_message_size(mut self, limit: usize) -> Self {
            self.max_encoding_message_size = Some(limit);
            self
        }
    }
    impl<T, B> tonic::codegen::Service<http::Request<B>> for TokenClassificationServer<T>
    where
        T: TokenClassification,
        B: Body + std::marker::Send + 'static,
        B::Error: Into<StdError> + std::marker::Send + 'static,
    {
        type Response = http::Response<tonic::body::Body>;
        type Error = std::convert::Infallible;
        type Future = BoxFuture<Self::Response, Self::Error>;
        fn poll_ready(
            &mut self,
            _cx: &mut Context<'_>,
        ) -> Poll<std::result::Result<(), Self::Error>> {
            Poll::Ready(Ok(()))
        }
        fn call(&mut self, req: http::Request<B>) -> Self::Future {
            match req.uri().path() {
                "/encoderfile.TokenClassification/Predict" => {
                    #[allow(non_camel_case_types)]
                    struct PredictSvc<T: TokenClassification>(pub Arc<T>);
                    impl<
                        T: TokenClassification,
                    > tonic::server::UnaryService<
                        super::super::token_classification::TokenClassificationRequest,
                    > for PredictSvc<T> {
                        type Response = super::super::token_classification::TokenClassificationResponse;
                        type Future = BoxFuture<
                            tonic::Response<Self::Response>,
                            tonic::Status,
                        >;
                        fn call(
                            &mut self,
                            request: tonic::Request<
                                super::super::token_classification::TokenClassificationRequest,
                            >,
                        ) -> Self::Future {
                            let inner = Arc::clone(&self.0);
                            let fut = async move {
                                <T as TokenClassification>::predict(&inner, request).await
                            };
                            Box::pin(fut)
                        }
                    }
                    let accept_compression_encodings = self.accept_compression_encodings;
                    let send_compression_encodings = self.send_compression_encodings;
                    let max_decoding_message_size = self.max_decoding_message_size;
                    let max_encoding_message_size = self.max_encoding_message_size;
                    let inner = self.inner.clone();
                    let fut = async move {
                        let method = PredictSvc(inner);
                        let codec = tonic_prost::ProstCodec::default();
                        let mut grpc = tonic::server::Grpc::new(codec)
                            .apply_compression_config(
                                accept_compression_encodings,
                                send_compression_encodings,
                            )
                            .apply_max_message_size_config(
                                max_decoding_message_size,
                                max_encoding_message_size,
                            );
                        let res = grpc.unary(method, req).await;
                        Ok(res)
                    };
                    Box::pin(fut)
                }
                _ => {
                    Box::pin(async move {
                        let mut response = http::Response::new(
                            tonic::body::Body::default(),
                        );
                        let headers = response.headers_mut();
                        headers
                            .insert(
                                tonic::Status::GRPC_STATUS,
                                (tonic::Code::Unimplemented as i32).into(),
                            );
                        headers
                            .insert(
                                http::header::CONTENT_TYPE,
                                tonic::metadata::GRPC_CONTENT_TYPE,
                            );
                        Ok(response)
                    })
                }
            }
        }
    }
    impl<T> Clone for TokenClassificationServer<T> {
        fn clone(&self) -> Self {
            let inner = self.inner.clone();
            Self {
                inner,
                accept_compression_encodings: self.accept_compression_encodings,
                send_compression_encodings: self.send_compression_encodings,
                max_decoding_message_size: self.max_decoding_message_size,
                max_encoding_message_size: self.max_encoding_message_size,
            }
        }
    }
    /// Generated gRPC service name
    pub const SERVICE_NAME: &str = "encoderfile.TokenClassification";
    impl<T> tonic::server::NamedService for TokenClassificationServer<T> {
        const NAME: &'static str = SERVICE_NAME;
    }
}
#[rustfmt::skip]
pub mod encoderfile;
#[rustfmt::skip]
pub mod embedding;
#[rustfmt::skip]
pub mod sequence_classification;
#[rustfmt::skip]
pub mod token_classification;
#[rustfmt::skip]
pub mod token;
// This file is @generated by prost-build.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SequenceClassificationRequest {
    #[prost(string, repeated, tag = "1")]
    pub inputs: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    #[prost(map = "string, string", tag = "2")]
    pub metadata: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SequenceClassificationResponse {
    #[prost(message, repeated, tag = "1")]
    pub results: ::prost::alloc::vec::Vec<SequenceClassificationResult>,
    #[prost(string, tag = "2")]
    pub model_id: ::prost::alloc::string::String,
    #[prost(map = "string, string", tag = "3")]
    pub metadata: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct SequenceClassificationResult {
    #[prost(float, repeated, tag = "1")]
    pub logits: ::prost::alloc::vec::Vec<f32>,
    #[prost(float, repeated, tag = "2")]
    pub scores: ::prost::alloc::vec::Vec<f32>,
    #[prost(uint32, tag = "3")]
    pub predicted_index: u32,
    #[prost(string, optional, tag = "4")]
    pub predicted_label: ::core::option::Option<::prost::alloc::string::String>,
}
// This file is @generated by prost-build.
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TokenClassificationRequest {
    #[prost(string, repeated, tag = "1")]
    pub inputs: ::prost::alloc::vec::Vec<::prost::alloc::string::String>,
    #[prost(map = "string, string", tag = "2")]
    pub metadata: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TokenClassificationResponse {
    #[prost(message, repeated, tag = "1")]
    pub results: ::prost::alloc::vec::Vec<TokenClassificationResult>,
    #[prost(string, tag = "2")]
    pub model_id: ::prost::alloc::string::String,
    #[prost(map = "string, string", tag = "3")]
    pub metadata: ::std::collections::HashMap<
        ::prost::alloc::string::String,
        ::prost::alloc::string::String,
    >,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TokenClassificationResult {
    #[prost(message, repeated, tag = "1")]
    pub tokens: ::prost::alloc::vec::Vec<TokenClassification>,
}
#[derive(Clone, PartialEq, ::prost::Message)]
pub struct TokenClassification {
    #[prost(message, optional, tag = "1")]
    pub token_info: ::core::option::Option<super::token::TokenInfo>,
    #[prost(float, repeated, tag = "2")]
    pub logits: ::prost::alloc::vec::Vec<f32>,
    #[prost(float, repeated, tag = "3")]
    pub scores: ::prost::alloc::vec::Vec<f32>,
    #[prost(string, tag = "4")]
    pub label: ::prost::alloc::string::String,
    #[prost(float, tag = "5")]
    pub score: f32,
}
// This file is @generated by prost-build.
#[derive(Clone, PartialEq, Eq, Hash, ::prost::Message)]
pub struct TokenInfo {
    #[prost(string, tag = "1")]
    pub token: ::prost::alloc::string::String,
    #[prost(uint32, tag = "2")]
    pub token_id: u32,
    #[prost(uint32, tag = "3")]
    pub start: u32,
    #[prost(uint32, tag = "4")]
    pub end: u32,
}
use crate::{
    config::{ModelType, get_model_type},
    generated::{
        embedding,
        encoderfile::{
            embedding_server::{Embedding, EmbeddingServer},
            sequence_classification_server::{
                SequenceClassification, SequenceClassificationServer,
            },
            token_classification_server::{TokenClassification, TokenClassificationServer},
        },
        sequence_classification, token_classification,
    },
};

pub fn router() -> axum::Router {
    let builder = tonic::service::Routes::builder().routes();

    match get_model_type() {
        ModelType::Embedding => builder.add_service(EmbeddingServer::new(EmbeddingService)),
        ModelType::SequenceClassification => builder.add_service(
            SequenceClassificationServer::new(SequenceClassificationService),
        ),
        ModelType::TokenClassification => {
            builder.add_service(TokenClassificationServer::new(TokenClassificationService))
        }
    }
    .into_axum_router()
}

macro_rules! generate_grpc_server {
    ($service_name:ident, $request_path:path, $response_path:path, $trait_path:path, $fn_path:path) => {
        #[derive(Debug, Default)]
        pub struct $service_name;

        #[tonic::async_trait]
        impl $trait_path for $service_name {
            async fn predict(
                &self,
                request: tonic::Request<$request_path>,
            ) -> Result<tonic::Response<$response_path>, tonic::Status> {
                Ok(tonic::Response::new(
                    $fn_path(request.into_inner())
                        .map_err(|e| e.to_tonic_status())?
                        .into(),
                ))
            }
        }
    };
}

generate_grpc_server!(
    EmbeddingService,
    embedding::EmbeddingRequest,
    embedding::EmbeddingResponse,
    Embedding,
    crate::services::embedding
);

generate_grpc_server!(
    SequenceClassificationService,
    sequence_classification::SequenceClassificationRequest,
    sequence_classification::SequenceClassificationResponse,
    SequenceClassification,
    crate::services::sequence_classification
);

generate_grpc_server!(
    TokenClassificationService,
    token_classification::TokenClassificationRequest,
    token_classification::TokenClassificationResponse,
    TokenClassification,
    crate::services::token_classification
);
use axum::{
    Json,
    response::IntoResponse,
    routing::{get, post},
};

use crate::{
    config::{ModelType, get_model_type},
    services,
};

const PREDICT_ROUTE: &'static str = "/predict";

pub fn router() -> axum::Router {
    let router = axum::Router::new().route("/health", get(|| async { "OK" }));

    match get_model_type() {
        ModelType::Embedding => router.route(PREDICT_ROUTE, post(embedding)),
        ModelType::SequenceClassification => {
            router.route(PREDICT_ROUTE, post(sequence_classification))
        }
        ModelType::TokenClassification => router.route(PREDICT_ROUTE, post(token_classification)),
    }
}

macro_rules! generate_route {
    ($fn_name:ident, $request_path:path, $fn_path:path) => {
        async fn $fn_name(Json(req): Json<$request_path>) -> impl IntoResponse {
            $fn_path(req)
                .map(|r| Json(r))
                .map_err(|e| e.to_axum_status())
        }
    };
}

generate_route!(embedding, services::EmbeddingRequest, services::embedding);
generate_route!(
    sequence_classification,
    services::SequenceClassificationRequest,
    services::sequence_classification
);
generate_route!(
    token_classification,
    services::TokenClassificationRequest,
    services::token_classification
);
use ndarray::{Axis, Ix2};
use tokenizers::Encoding;

use crate::{error::ApiError, inference::utils::requires_token_type_ids};

pub fn embedding<'a>(
    mut session: super::model::Model<'a>,
    encodings: Vec<Encoding>,
    normalize: bool,
) -> Result<Vec<TokenEmbeddingSequence>, ApiError> {
    let (a_ids, a_mask, a_type_ids) = crate::prepare_inputs!(encodings);

    let outputs = match requires_token_type_ids(&session) {
        true => session.run(ort::inputs!(a_ids, a_mask, a_type_ids)),
        false => session.run(ort::inputs!(a_ids, a_mask)),
    }
    .map_err(|e| {
        tracing::error!("Error running model: {:?}", e);
        ApiError::InternalError("Error running model")
    })?;

    let outputs = outputs
        .get("last_hidden_state")
        .unwrap()
        .try_extract_array::<f32>()
        .unwrap();

    let mut embeddings = Vec::new();

    for (encoding, embs) in encodings.iter().zip(outputs.axis_iter(Axis(0))) {
        let mut transformed = embs.into_dimensionality::<Ix2>().unwrap().into_owned();

        if normalize {
            transformed = super::utils::l2_normalize(transformed, Axis(1));
        }

        let mut token_ids = encoding.get_ids().iter();
        let mut tokens = encoding.get_tokens().iter();
        let mut special_tokens_mask = encoding.get_special_tokens_mask().iter();
        let mut offsets = encoding.get_offsets().iter();
        let mut embeddings_iter = transformed.axis_iter(Axis(0));

        let mut results = Vec::new();

        while let (Some(token_id), Some(token), Some(special_tokens_mask), Some(offset), Some(e)) = (
            token_ids.next(),
            tokens.next(),
            special_tokens_mask.next(),
            offsets.next(),
            embeddings_iter.next(),
        ) {
            if *special_tokens_mask == 1 {
                continue;
            }

            let (start, end) = *offset;
            let embedding: Vec<f32> = e.iter().map(|i| *i).collect();

            let token_info = super::token_info::TokenInfo {
                token: token.clone(),
                token_id: *token_id,
                start,
                end,
            };

            results.push(TokenEmbedding {
                embedding,
                token_info: Some(token_info),
            })
        }

        embeddings.push(results)
    }

    Ok(embeddings)

    // Err(ApiError::InternalError("Not Implemented"))
}

#[derive(Debug, serde::Serialize)]
pub struct TokenEmbedding {
    pub embedding: Vec<f32>,
    pub token_info: Option<super::token_info::TokenInfo>,
}

impl From<TokenEmbedding> for crate::generated::embedding::TokenEmbedding {
    fn from(val: TokenEmbedding) -> Self {
        crate::generated::embedding::TokenEmbedding {
            embedding: val.embedding,
            token_info: val.token_info.map(|i| i.into()),
        }
    }
}

pub type TokenEmbeddingSequence = Vec<TokenEmbedding>;

impl From<TokenEmbeddingSequence> for crate::generated::embedding::TokenEmbeddingSequence {
    fn from(val: Vec<TokenEmbedding>) -> Self {
        Self {
            embeddings: val.into_iter().map(|i| i.into()).collect(),
        }
    }
}
pub mod embedding;
pub mod model;
pub mod sequence_classification;
pub mod token_classification;
pub mod tokenizer;
pub mod utils;

pub mod token_info {
    #[derive(Debug, serde::Serialize, serde::Deserialize)]
    pub struct TokenInfo {
        pub token: String,
        pub token_id: u32,
        pub start: usize,
        pub end: usize,
    }

    impl From<TokenInfo> for crate::generated::token::TokenInfo {
        fn from(val: TokenInfo) -> Self {
            crate::generated::token::TokenInfo {
                token: val.token,
                token_id: val.token_id,
                start: (val.start as u32),
                end: (val.end as u32),
            }
        }
    }
}
use ort::session::Session;
use parking_lot::{Mutex, MutexGuard};
use std::sync::OnceLock;

use crate::assets::MODEL_WEIGHTS;

static MODEL: OnceLock<Mutex<Session>> = OnceLock::new();

pub type Model<'a> = MutexGuard<'a, Session>;

pub fn get_model() -> Model<'static> {
    let model = MODEL.get_or_init(|| {
        match Session::builder().and_then(|s| s.commit_from_memory(MODEL_WEIGHTS)) {
            Ok(model) => Mutex::new(model),
            Err(e) => panic!("FATAL: Failed to load model: {e:?}"),
        }
    });

    model.lock()
}
use crate::{config::get_model_config, error::ApiError, inference::utils::requires_token_type_ids};
use ndarray::{Axis, Ix2};
use ndarray_stats::QuantileExt;
use tokenizers::Encoding;

pub fn sequence_classification<'a>(
    mut session: super::model::Model<'a>,
    encodings: Vec<Encoding>,
) -> Result<Vec<SequenceClassificationResult>, ApiError> {
    let (a_ids, a_mask, a_type_ids) = crate::prepare_inputs!(encodings);

    let outputs = match requires_token_type_ids(&session) {
        true => session.run(ort::inputs!(a_ids, a_mask, a_type_ids)),
        false => session.run(ort::inputs!(a_ids, a_mask)),
    }
    .map_err(|e| {
        tracing::error!("Error running model: {:?}", e);
        ApiError::InternalError("Error running model")
    })?;

    // get logits
    // will be in shape [N, L]
    let outputs = outputs
        .get("logits")
        .unwrap()
        .try_extract_array::<f32>()
        .unwrap()
        .into_dimensionality::<Ix2>()
        .unwrap()
        .into_owned();

    let probabilities = super::utils::softmax(&outputs, Axis(1));

    let model_config = get_model_config();

    let results = outputs
        .axis_iter(Axis(0))
        .zip(probabilities.axis_iter(Axis(0)))
        .map(|(logs, probs)| {
            let predicted_index = probs.argmax().unwrap();
            SequenceClassificationResult {
                logits: logs.iter().map(|i| *i).collect(),
                scores: probs.iter().map(|i| *i).collect(),
                predicted_index: (predicted_index as u32),
                predicted_label: model_config
                    .id2label(predicted_index as u32)
                    .map(|i| i.to_string()),
            }
        })
        .collect();

    Ok(results)
}

#[derive(Debug, serde::Serialize)]
pub struct SequenceClassificationResult {
    pub logits: Vec<f32>,
    pub scores: Vec<f32>,
    pub predicted_index: u32,
    pub predicted_label: Option<String>,
}

impl From<SequenceClassificationResult>
    for crate::generated::sequence_classification::SequenceClassificationResult
{
    fn from(val: SequenceClassificationResult) -> Self {
        Self {
            logits: val.logits,
            scores: val.scores,
            predicted_index: val.predicted_index,
            predicted_label: val.predicted_label,
        }
    }
}
use crate::{config::get_model_config, error::ApiError, inference::utils::softmax};
use ndarray::{Axis, Ix2};
use ndarray_stats::QuantileExt;
use tokenizers::Encoding;

pub fn token_classification<'a>(
    mut session: super::model::Model<'a>,
    encodings: Vec<Encoding>,
) -> Result<Vec<TokenClassificationResult>, ApiError> {
    let (a_ids, a_mask, a_type_ids) = crate::prepare_inputs!(encodings);

    let outputs = match super::utils::requires_token_type_ids(&session) {
        true => session.run(ort::inputs!(a_ids, a_mask, a_type_ids)),
        false => session.run(ort::inputs!(a_ids, a_mask)),
    }
    .map_err(|e| {
        tracing::error!("Error running model: {:?}", e);
        ApiError::InternalError("Error running model")
    })?;

    let outputs = outputs
        .get("logits")
        .unwrap()
        .try_extract_array::<f32>()
        .unwrap();

    let mut predictions = Vec::new();

    for (encoding, logits) in encodings.iter().zip(outputs.axis_iter(Axis(0))) {
        let logits = logits.into_dimensionality::<Ix2>().unwrap().to_owned();

        let scores = softmax(&logits, Axis(1));

        let mut token_ids = encoding.get_ids().iter();
        let mut tokens = encoding.get_tokens().iter();
        let mut special_tokens_mask = encoding.get_special_tokens_mask().iter();
        let mut offsets = encoding.get_offsets().iter();
        let mut logs_iter = logits.axis_iter(Axis(0));
        let mut scores_iter = scores.axis_iter(Axis(0));

        let mut results = Vec::new();

        let model_config = get_model_config();

        while let (
            Some(token_id),
            Some(token),
            Some(special_tokens_mask),
            Some(offset),
            Some(logs),
            Some(scores),
        ) = (
            token_ids.next(),
            tokens.next(),
            special_tokens_mask.next(),
            offsets.next(),
            logs_iter.next(),
            scores_iter.next(),
        ) {
            let argmax = scores.argmax().unwrap();
            let score = scores[argmax];
            let label = model_config.id2label(argmax as u32).unwrap().to_string();

            let (start, end) = *offset;

            if *special_tokens_mask == 1 {
                continue;
            }

            results.push(TokenClassification {
                token_info: super::token_info::TokenInfo {
                    token_id: *token_id,
                    token: token.clone(),
                    start,
                    end,
                },
                score: score,
                label,
                logits: logs.iter().map(|i| *i).collect(),
                scores: scores.iter().map(|i| *i).collect(),
            })
        }

        predictions.push(results);
    }

    Ok(predictions)
}

pub type TokenClassificationResult = Vec<TokenClassification>;

impl From<TokenClassificationResult>
    for crate::generated::token_classification::TokenClassificationResult
{
    fn from(val: TokenClassificationResult) -> Self {
        Self {
            tokens: val.into_iter().map(|i| i.into()).collect(),
        }
    }
}

#[derive(Debug, serde::Serialize)]
pub struct TokenClassification {
    pub token_info: super::token_info::TokenInfo,
    pub logits: Vec<f32>,
    pub scores: Vec<f32>,
    pub label: String,
    pub score: f32,
}

impl From<TokenClassification> for crate::generated::token_classification::TokenClassification {
    fn from(val: TokenClassification) -> Self {
        Self {
            token_info: Some(val.token_info.into()),
            logits: val.logits,
            scores: val.scores,
            label: val.label,
            score: val.score,
        }
    }
}
use crate::{assets::TOKENIZER_JSON, config::get_model_config, error::ApiError};
use anyhow::Result;
use std::str::FromStr;
use std::sync::OnceLock;
use tokenizers::{
    Encoding, PaddingDirection, PaddingParams, PaddingStrategy, tokenizer::Tokenizer,
};

static TOKENIZER: OnceLock<Tokenizer> = OnceLock::new();

pub fn get_tokenizer() -> &'static Tokenizer {
    let model_config = get_model_config();
    let pad_token_id = model_config.pad_token_id;

    TOKENIZER.get_or_init(|| {
        let mut tokenizer = match Tokenizer::from_str(TOKENIZER_JSON) {
            Ok(t) => t,
            Err(e) => panic!("FATAL: Error loading tokenizer: {:?}", e),
        };

        let pad_token = match tokenizer.id_to_token(pad_token_id) {
            Some(tok) => tok,
            None => panic!("Model requires a padding token."),
        };

        if tokenizer.get_padding().is_none() {
            let params = PaddingParams {
                strategy: PaddingStrategy::BatchLongest,
                direction: PaddingDirection::Right,
                pad_to_multiple_of: None,
                pad_id: pad_token_id,
                pad_type_id: 0,
                pad_token,
            };

            tracing::warn!(
                "No padding strategy specified in tokenizer config. Setting default: {:?}",
                &params
            );
            tokenizer.with_padding(Some(params));
        }

        tokenizer
    })
}

pub fn encode_text(tokenizer: &Tokenizer, text: Vec<String>) -> Result<Vec<Encoding>, ApiError> {
    if text.is_empty() || text.iter().any(|i| i.is_empty()) {
        return Err(ApiError::InputError("Cannot tokenize empty string"));
    }

    tokenizer.encode_batch(text, true).map_err(|e| {
        tracing::error!("Error tokenizing text: {}", e);
        ApiError::InternalError("Error during tokenization")
    })
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_get_tokenizer_initializes_once() {
        let tokenizer1 = get_tokenizer();
        let tokenizer2 = get_tokenizer();

        // Should point to the same static instance
        let ptr1 = tokenizer1 as *const _;
        let ptr2 = tokenizer2 as *const _;
        assert_eq!(ptr1, ptr2, "Tokenizers should be the same instance");
    }

    #[test]
    fn test_encode_text_basic() {
        let text = "Hello world!".to_string();
        let tokenizer = get_tokenizer();
        let encoding = encode_text(tokenizer, vec![text.clone()])
            .expect("failed to encode text")
            .first()
            .expect("nothing encoded?")
            .clone();

        assert!(
            encoding.len() <= 16,
            "Encoding length should not exceed padding target"
        );
        assert!(encoding.get_tokens().len() <= 16);
        assert!(
            encoding
                .get_tokens()
                .iter()
                .any(|t| t.contains("hello") || t.contains("world")),
            "Tokens should contain text fragments"
        );
    }
}
use ndarray::{Array2, Axis};
use ort::session::Session;
use parking_lot::MutexGuard;

#[macro_export]
macro_rules! prepare_inputs {
    ($encodings:ident) => {{
        let padded_token_length = $encodings[0].len();

        // Get token IDs & mask as a flattened array.
        let ids: Vec<i64> = $encodings
            .iter()
            .flat_map(|e| e.get_ids().iter().map(|i| *i as i64))
            .collect();
        let mask: Vec<i64> = $encodings
            .iter()
            .flat_map(|e| e.get_attention_mask().iter().map(|i| *i as i64))
            .collect();
        let type_ids: Vec<i64> = $encodings
            .iter()
            .flat_map(|e| e.get_type_ids().iter().map(|i| *i as i64))
            .collect();

        // Convert flattened arrays into 2-dimensional tensors of shape [N, L].
        let a_ids = ort::value::TensorRef::from_array_view((
            [$encodings.len(), padded_token_length],
            &*ids,
        ))
        .unwrap()
        .to_owned();
        let a_mask = ort::value::TensorRef::from_array_view((
            [$encodings.len(), padded_token_length],
            &*mask,
        ))
        .unwrap()
        .to_owned();
        let a_type_ids = ort::value::TensorRef::from_array_view((
            [$encodings.len(), padded_token_length],
            &*type_ids,
        ))
        .unwrap()
        .to_owned();

        (a_ids, a_mask, a_type_ids)
    }};
}

pub fn l2_normalize(mut x: Array2<f32>, axis: Axis) -> Array2<f32> {
    for mut row in x.axis_iter_mut(axis) {
        let norm = row.mapv(|v| v * v).sum().sqrt();
        if norm > 0.0 {
            row.mapv_inplace(|v| v / norm);
        }
    }
    x
}

pub fn softmax(x: &Array2<f32>, axis: Axis) -> Array2<f32> {
    let max_per_axis = x.map_axis(axis, |row| row.fold(f32::NEG_INFINITY, |a, &b| a.max(b)));
    let expx = x - &max_per_axis.insert_axis(axis);
    let expx = expx.mapv(f32::exp);
    let sum = expx.sum_axis(axis).insert_axis(axis);
    &expx / &sum
}

pub fn requires_token_type_ids<'a>(session: &MutexGuard<'a, Session>) -> bool {
    session
        .inputs
        .iter()
        .any(|inp| inp.name == "token_type_ids")
}
mod assets;
pub mod cli;
pub mod config;
pub mod error;
pub mod generated;
pub mod grpc;
pub mod http;
pub mod inference;
pub mod services;

pub use assets::get_banner;
use anyhow::Result;
use clap::Parser;
use encoderfile::{
    cli::Commands,
    config::{ModelType, get_model_type},
    error::ApiError,
    services::{
        EmbeddingRequest, SequenceClassificationRequest, TokenClassificationRequest, embedding,
        sequence_classification, token_classification,
    },
};
use serde_json::json;
use tracing_subscriber::EnvFilter;

macro_rules! generate_cli_route {
    ($req:ident, $fn:path) => {
        match $fn($req) {
            Ok(r) => println!("{}", json!(r).to_string()),
            Err(e) => println!("{}", json!(e).to_string()),
        }
    };
}

#[tokio::main]
async fn main() -> Result<()> {
    tracing_subscriber::fmt()
        .with_env_filter(
            EnvFilter::try_from_default_env().unwrap_or_else(|_| EnvFilter::new("info")), // default to "info" level
        )
        .with_target(false) // hide module path
        .compact() // short, pretty output
        .init();

    let cli = encoderfile::cli::Cli::parse();

    match cli.command {
        Commands::Serve {
            grpc_hostname,
            grpc_port,
            http_hostname,
            http_port,
            disable_grpc,
            disable_http,
        } => {
            if disable_grpc && disable_http {
                return Err(ApiError::ConfigError("Cannot disable both gRPC and HTTP"))?;
            }

            let grpc_process = match disable_grpc {
                true => tokio::spawn(async { Ok(()) }),
                false => tokio::spawn(run_grpc(grpc_hostname, grpc_port)),
            };

            let http_process = match disable_http {
                true => tokio::spawn(async { Ok(()) }),
                false => tokio::spawn(run_http(http_hostname, http_port)),
            };

            println!("{}", encoderfile::get_banner());

            let _ = tokio::join!(grpc_process, http_process);
        }
        Commands::Infer { inputs, normalize } => {
            let metadata = None;

            match get_model_type() {
                ModelType::Embedding => {
                    let request = EmbeddingRequest {
                        inputs,
                        normalize,
                        metadata,
                    };

                    generate_cli_route!(request, embedding)
                }
                ModelType::SequenceClassification => {
                    let request = SequenceClassificationRequest { inputs, metadata };

                    generate_cli_route!(request, sequence_classification)
                }
                ModelType::TokenClassification => {
                    let request = TokenClassificationRequest { inputs, metadata };

                    generate_cli_route!(request, token_classification)
                }
            }
        }
    }

    Ok(())
}

async fn run_grpc(hostname: String, port: String) -> Result<()> {
    let addr = format!("{}:{}", &hostname, &port);

    let router = encoderfile::grpc::router()
        .layer(tower_http::trace::TraceLayer::new_for_grpc())
        .into_make_service_with_connect_info::<std::net::SocketAddr>();

    let listener = tokio::net::TcpListener::bind(&addr).await.unwrap();

    tracing::info!("Running {:?} gRPC server on {}", get_model_type(), &addr);

    axum::serve(listener, router).await?;

    Ok(())
}

async fn run_http(hostname: String, port: String) -> Result<()> {
    let addr = format!("{}:{}", &hostname, &port);

    let router = encoderfile::http::router()
        .layer(tower_http::trace::TraceLayer::new_for_http())
        .into_make_service_with_connect_info::<std::net::SocketAddr>();

    let listener = tokio::net::TcpListener::bind(&addr).await.unwrap();

    tracing::info!("Running {:?} HTTP server on {}", get_model_type(), &addr);

    axum::serve(listener, router).await?;

    Ok(())
}
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

use crate::{
    error::ApiError,
    inference::{self, embedding::TokenEmbedding, model::get_model, tokenizer::get_tokenizer},
};

pub fn embedding(request: impl Into<EmbeddingRequest>) -> Result<EmbeddingResponse, ApiError> {
    let request = request.into();

    let tokenizer = get_tokenizer();
    let session = get_model();

    let encodings = inference::tokenizer::encode_text(tokenizer, request.inputs)?;

    let results = inference::embedding::embedding(session, encodings, request.normalize)?;

    Ok(EmbeddingResponse {
        results,
        model_id: crate::assets::MODEL_ID.to_string(),
        metadata: request.metadata,
    })
}

#[derive(Debug, Deserialize)]
pub struct EmbeddingRequest {
    pub inputs: Vec<String>,
    pub normalize: bool,
    #[serde(default)]
    pub metadata: Option<HashMap<String, String>>,
}

impl From<crate::generated::embedding::EmbeddingRequest> for EmbeddingRequest {
    fn from(val: crate::generated::embedding::EmbeddingRequest) -> Self {
        Self {
            inputs: val.inputs,
            normalize: val.normalize,
            metadata: Some(val.metadata),
        }
    }
}

#[derive(Debug, Serialize)]
pub struct EmbeddingResponse {
    results: Vec<Vec<TokenEmbedding>>,
    model_id: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    metadata: Option<HashMap<String, String>>,
}

impl From<EmbeddingResponse> for crate::generated::embedding::EmbeddingResponse {
    fn from(val: EmbeddingResponse) -> Self {
        Self {
            results: val.results.into_iter().map(|embs| embs.into()).collect(),
            model_id: val.model_id,
            metadata: val.metadata.unwrap_or(HashMap::new()),
        }
    }
}
mod embedding;
mod sequence_classification;
mod token_classification;

pub use embedding::*;
pub use sequence_classification::*;
pub use token_classification::*;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

use crate::{
    error::ApiError,
    inference::{
        self, model::get_model, sequence_classification::SequenceClassificationResult,
        tokenizer::get_tokenizer,
    },
};

pub fn sequence_classification(
    request: impl Into<SequenceClassificationRequest>,
) -> Result<SequenceClassificationResponse, ApiError> {
    let request = request.into();
    let tokenizer = get_tokenizer();
    let session = get_model();

    let encodings = inference::tokenizer::encode_text(tokenizer, request.inputs)?;

    let results = inference::sequence_classification::sequence_classification(session, encodings)?;

    Ok(SequenceClassificationResponse {
        results,
        model_id: crate::assets::MODEL_ID.to_string(),
        metadata: request.metadata,
    })
}

#[derive(Debug, Deserialize)]
pub struct SequenceClassificationRequest {
    pub inputs: Vec<String>,
    #[serde(default)]
    pub metadata: Option<HashMap<String, String>>,
}

impl From<crate::generated::sequence_classification::SequenceClassificationRequest>
    for SequenceClassificationRequest
{
    fn from(val: crate::generated::sequence_classification::SequenceClassificationRequest) -> Self {
        Self {
            inputs: val.inputs,
            metadata: Some(val.metadata),
        }
    }
}

#[derive(Debug, Serialize)]
pub struct SequenceClassificationResponse {
    results: Vec<SequenceClassificationResult>,
    model_id: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    metadata: Option<HashMap<String, String>>,
}

impl From<SequenceClassificationResponse>
    for crate::generated::sequence_classification::SequenceClassificationResponse
{
    fn from(val: SequenceClassificationResponse) -> Self {
        Self {
            results: val.results.into_iter().map(|i| i.into()).collect(),
            model_id: val.model_id,
            metadata: val.metadata.unwrap_or(HashMap::new()),
        }
    }
}
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

use crate::{
    error::ApiError,
    inference::{
        self, model::get_model, token_classification::TokenClassificationResult,
        tokenizer::get_tokenizer,
    },
};

pub fn token_classification(
    request: impl Into<TokenClassificationRequest>,
) -> Result<TokenClassificationResponse, ApiError> {
    let request = request.into();
    let tokenizer = get_tokenizer();
    let session = get_model();

    let encodings = inference::tokenizer::encode_text(tokenizer, request.inputs)?;

    let results = inference::token_classification::token_classification(session, encodings)?;

    Ok(TokenClassificationResponse {
        results,
        model_id: crate::assets::MODEL_ID.to_string(),
        metadata: request.metadata,
    })
}

#[derive(Debug, Deserialize)]
pub struct TokenClassificationRequest {
    pub inputs: Vec<String>,
    #[serde(default)]
    pub metadata: Option<HashMap<String, String>>,
}

impl From<crate::generated::token_classification::TokenClassificationRequest>
    for TokenClassificationRequest
{
    fn from(val: crate::generated::token_classification::TokenClassificationRequest) -> Self {
        Self {
            inputs: val.inputs,
            metadata: Some(val.metadata),
        }
    }
}

#[derive(Debug, Serialize)]
pub struct TokenClassificationResponse {
    results: Vec<TokenClassificationResult>,
    model_id: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    metadata: Option<HashMap<String, String>>,
}

impl From<TokenClassificationResponse>
    for crate::generated::token_classification::TokenClassificationResponse
{
    fn from(val: TokenClassificationResponse) -> Self {
        Self {
            results: val.results.into_iter().map(|i| i.into()).collect(),
            model_id: crate::assets::MODEL_ID.to_string(),
            metadata: val.metadata.unwrap_or(HashMap::new()),
        }
    }
}
pub const BANNER: &'static str = include_str!("../assets/banner.txt");
